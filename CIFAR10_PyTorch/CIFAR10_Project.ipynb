{"cells":[{"cell_type":"markdown","metadata":{"id":"Ts664t76QNMq"},"source":["#<font color='blue' size='5px'/> CIFAR10 Project<font/>"],"id":"Ts664t76QNMq"},{"cell_type":"markdown","metadata":{"id":"xB4wLnm0QNNJ"},"source":["##  Problem Statement"],"id":"xB4wLnm0QNNJ"},{"cell_type":"markdown","source":["The CIFAR10 project aims to classify 60,000 32x32 color images into 10 different classes, including airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. The challenge is to develop an accurate image classification model that can correctly identify the object in the image despite variations in lighting, color, and orientation."],"metadata":{"id":"ZkjomOk1z526"},"id":"ZkjomOk1z526"},{"cell_type":"markdown","source":["## Packages"],"metadata":{"id":"hSoTjUEdqLRW"},"id":"hSoTjUEdqLRW"},{"cell_type":"code","source":["pip install torch torchvision"],"metadata":{"id":"6I7Dx8GMqRCX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694216965352,"user_tz":-180,"elapsed":4896,"user":{"displayName":"Abdelrahman Katkat","userId":"13800345600658892031"}},"outputId":"10bbc66c-4df7-4df3-c693-502aaba1a07f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.4.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}],"id":"6I7Dx8GMqRCX"},{"cell_type":"code","source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms ## For Transformation on Images"],"metadata":{"id":"rggZ8AtpqWs2"},"execution_count":null,"outputs":[],"id":"rggZ8AtpqWs2"},{"cell_type":"markdown","source":["## Dataloader Pipeline"],"metadata":{"id":"rhujII1JVZy4"},"id":"rhujII1JVZy4"},{"cell_type":"markdown","source":["### Data Transforms\n","Define transformations to preprocess the data. Common transformations include resizing, normalizing, and converting data to PyTorch tensors. For example:"],"metadata":{"id":"LbXr0Mq8qhMv"},"id":"LbXr0Mq8qhMv"},{"cell_type":"code","source":["transform = transforms.Compose([\n","    transforms.Resize((32, 32)),           # Resize images to 32x32 pixels\n","    transforms.ToTensor(),                # Convert images to PyTorch tensors\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize pixel values\n","])"],"metadata":{"id":"mv6L-lj5qrLd"},"execution_count":null,"outputs":[],"id":"mv6L-lj5qrLd"},{"cell_type":"markdown","source":["### Load Dataset"],"metadata":{"id":"zAP7PioFrM6k"},"id":"zAP7PioFrM6k"},{"cell_type":"markdown","source":["You can use PyTorch's torchvision.datasets module to download and load common datasets easily. For this example, we'll use CIFAR-10:"],"metadata":{"id":"kWtMgDMdrRC3"},"id":"kWtMgDMdrRC3"},{"cell_type":"code","source":["# Download and load the CIFAR-10 training dataset\n","train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n","\n","# Download and load the CIFAR-10 test dataset\n","test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dtrj972FrSpB","executionInfo":{"status":"ok","timestamp":1694216986889,"user_tz":-180,"elapsed":13965,"user":{"displayName":"Abdelrahman Katkat","userId":"13800345600658892031"}},"outputId":"f907e734-6f0c-446c-f14e-bb6db7dd8aa2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:06<00:00, 27883953.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"]}],"id":"Dtrj972FrSpB"},{"cell_type":"markdown","source":["### Create DataLoaders"],"metadata":{"id":"-UA7Wvaxr7hn"},"id":"-UA7Wvaxr7hn"},{"cell_type":"markdown","source":["Data loaders help you iterate through the dataset conveniently during training. You can specify batch sizes and enable shuffling of data to enhance training performance:"],"metadata":{"id":"mU6lL-y8sA5Q"},"id":"mU6lL-y8sA5Q"},{"cell_type":"markdown","source":["You should use the torch.utils.data.DataLoader class to create data loaders.\n","\n","- Ensure that you set the **num_workers** argument to utilize multiple CPU cores for data loading, which can significantly speed up the process.\n","- Also, set the shuffle argument to **True** for the training data loader to randomize the order of samples during training. For the test data loader, set shuffle to **False**."],"metadata":{"id":"ip6hCXD3tFSA"},"id":"ip6hCXD3tFSA"},{"cell_type":"code","source":["# Create data loaders\n","batch_size = 64\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"],"metadata":{"id":"cQE0HKy8sAZF"},"execution_count":null,"outputs":[],"id":"cQE0HKy8sAZF"},{"cell_type":"code","source":["# Iterate through the training data loader\n","for images, labels in train_loader:\n","    # Your training code here\n","    print(\"Batch of images shape:\", images.shape)\n","    print(\"Batch of labels:\", labels)\n","    break  # Break after processing the first batch for this example\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MI-Q1ko5sZfL","executionInfo":{"status":"ok","timestamp":1694216992478,"user_tz":-180,"elapsed":371,"user":{"displayName":"Abdelrahman Katkat","userId":"13800345600658892031"}},"outputId":"af2c2c6e-9e42-4507-ed33-e9e0fd17fc89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Batch of images shape: torch.Size([64, 3, 32, 32])\n","Batch of labels: tensor([2, 1, 6, 7, 2, 0, 4, 4, 2, 3, 5, 0, 4, 6, 4, 8, 0, 5, 2, 2, 1, 3, 8, 6,\n","        4, 2, 3, 7, 5, 1, 3, 4, 9, 8, 0, 6, 7, 8, 7, 3, 1, 3, 1, 3, 0, 2, 8, 3,\n","        8, 3, 0, 5, 8, 1, 6, 1, 2, 9, 4, 0, 7, 3, 4, 1])\n"]}],"id":"MI-Q1ko5sZfL"},{"cell_type":"markdown","metadata":{"id":"fYJLlWH7QNOc"},"source":["##  Model Selection & Training"],"id":"fYJLlWH7QNOc"},{"cell_type":"markdown","source":["### Moidel Building"],"metadata":{"id":"tYS7eVdrbvxT"},"id":"tYS7eVdrbvxT"},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim"],"metadata":{"id":"MPbZRkR3N3SB"},"id":"MPbZRkR3N3SB","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- **Model Overview:**\n","\n","  The following neural network model consists of two convolutional layers (`conv1` and `conv2`) that transform the input tensor into a higher-dimensional feature space. Specifically, `conv1` takes an input tensor with shape `(batch_size, 3, 32, 32)` and produces an output tensor with shape `(batch_size, 16, 32, 32)`, while `conv2` takes an input tensor with shape `(batch_size, 16, 16, 16)` and produces an output tensor with shape `(batch_size, 32, 8, 8)`.\n","\n","  The output tensor of the second convolutional layer (`conv2`) is then flattened into a one-dimensional vector of length `32 x 8 x 8` using the `view` method. This one-dimensional vector is then passed through two fully connected layers (`fc1` and `fc2`) that transform the vector into a 10-dimensional output tensor.\n","\n","  Each layer in the neural network model learns to extract and transform features from the input tensor to produce a higher-level representation of the data. By chaining multiple layers together in this way, the neural network is able to learn increasingly complex representations of the data that are useful for the task at hand (in this case, image classification on the CIFAR10 dataset).\n","\n","- **View In the model**\n","\n","  - The `view` method in PyTorch is used to reshape a tensor while preserving its total number of elements. In this case, the `-1` argument in `x.view(-1, 32 * 8 * 8)` indicates that the size of that dimension should be inferred based on the other dimensions and the total number of elements in the tensor.\n","  - `x = x.view(-1, 32 * 8 * 8)` is used to reshape the output tensor of the second convolutional layer (`conv2`) into a one-dimensional vector.\n","\n","  - The output tensor of `conv2` has a shape of `(batch_size, 32, 8, 8)`, where `batch_size` is the number of samples in the batch. By reshaping it using `x.view(-1, 32 * 8 * 8)`, we flatten the tensor into a one-dimensional vector of length `32 * 8 * 8`. The `-1` argument allows the batch size to be automatically inferred based on the original shape of the tensor.\n","\n","  - This reshaping step is necessary because the subsequent fully connected layers (`fc1` and `fc2`) expect a **one-dimensional input tensor**. By flattening the tensor, we ensure that the output of `conv2` can be properly fed into the fully connected layers for further processing.\n","\n","  - The `x.view(-1, 32 * 8 * 8)` operation in PyTorch is equivalent to the `tf.reshape(x, [-1, 32 * 8 * 8])` operation in TensorFlow, **which flattens a tensor into a one-dimensional vector while preserving its total number of elements**.\n","\n"],"metadata":{"id":"4oIGNrMkUpua"},"id":"4oIGNrMkUpua"},{"cell_type":"code","source":["class MyModel(nn.Module):\n","    def __init__(self):\n","        super(MyModel, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","\n","        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n","\n","\n","        ## The in_features parameter specifies the number of input features to the layer. In this case, the input tensor to the fully connected layer has shape (batch_size, 32, 8, 8)\n","        ## where batch_size is the number of samples in the batch,\n","        ## 32 is the number of output channels from the previous convolutional layer,\n","        ## 8 x 8 is the spatial size of the feature maps.\n","        ##The in_features parameter is set to 32 x 8 x 8 to reflect the fact that each feature map is flattened into a one-dimensional vector of length 32 x 8 x 8 before being passed to the fully connected layer.\n","        self.fc1 = nn.Linear(in_features=32 * 8 * 8, out_features=64)\n","        self.fc2 = nn.Linear(in_features=64, out_features=10)\n","\n","    def forward(self, x):\n","        ## The reason for this difference is that torch.relu is a built-in function in PyTorch that applies the ReLU activation function element-wise to a tensor\n","        ## while self.pool is an instance of the nn.MaxPool2d class that performs max pooling on a tensor.\n","        x = torch.relu(self.conv1(x))\n","        x = self.pool(x)\n","\n","        x = torch.relu(self.conv2(x))\n","        x = self.pool(x)\n","\n","        ## The view is similar to Flatten\n","        x = x.view(-1, 32 * 8 * 8)\n","\n","        ## self.fc1 is an instance of the nn.Linear class, we need to call it as a method of the instance (self.fc1(x)) in order to apply the linear transformation to the tensor.\n","        x = torch.relu(self.fc1(x))\n","\n","        ## self.fc2 is an instance of the nn.Linear class, we need to call it as a method of the instance (self.fc2(x)) in order to apply the linear transformation to the tensor.\n","        x = self.fc2(x)\n","        return x"],"metadata":{"id":"5TxQc6zzYKbx"},"id":"5TxQc6zzYKbx","execution_count":null,"outputs":[]},{"cell_type":"code","source":["model=MyModel()"],"metadata":{"id":"cVoScwHmX_G-"},"id":"cVoScwHmX_G-","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchsummary import summary\n","\n","summary(model, (64, 3, 32, 32))"],"metadata":{"id":"S6KuJ1d9fUOA"},"id":"S6KuJ1d9fUOA","execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"],"metadata":{"id":"2ugnfin7fKEH"},"id":"2ugnfin7fKEH","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Optimizer & Loss Function"],"metadata":{"id":"xmvTcxdOb1Om"},"id":"xmvTcxdOb1Om"},{"cell_type":"code","source":["# Define the loss function and optimizer\n","model = MyModel()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"],"metadata":{"id":"nvJ-6a5lYm8v"},"id":"nvJ-6a5lYm8v","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","num_epochs = 10"],"metadata":{"id":"7GYitXWEY0sA"},"id":"7GYitXWEY0sA","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Train Model"],"metadata":{"id":"8Zc6zvKHb4__"},"id":"8Zc6zvKHb4__"},{"cell_type":"code","source":["for epoch in range(num_epochs):\n","    running_loss = 0.0\n","    for i, (inputs, labels) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        if i % 100 == 99:\n","            print('[Epoch %d, Batch %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 100))\n","            running_loss = 0.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oZTa6hGCY5WK","executionInfo":{"status":"ok","timestamp":1694220889383,"user_tz":-180,"elapsed":447273,"user":{"displayName":"Abdelrahman Katkat","userId":"13800345600658892031"}},"outputId":"45e4534d-8ed5-4a7f-82b0-a8854dee3441"},"id":"oZTa6hGCY5WK","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Epoch 1, Batch   100] loss: 1.944\n","[Epoch 1, Batch   200] loss: 1.657\n","[Epoch 1, Batch   300] loss: 1.503\n","[Epoch 1, Batch   400] loss: 1.454\n","[Epoch 1, Batch   500] loss: 1.383\n","[Epoch 1, Batch   600] loss: 1.344\n","[Epoch 1, Batch   700] loss: 1.287\n","[Epoch 2, Batch   100] loss: 1.209\n","[Epoch 2, Batch   200] loss: 1.173\n","[Epoch 2, Batch   300] loss: 1.137\n","[Epoch 2, Batch   400] loss: 1.135\n","[Epoch 2, Batch   500] loss: 1.127\n","[Epoch 2, Batch   600] loss: 1.107\n","[Epoch 2, Batch   700] loss: 1.090\n","[Epoch 3, Batch   100] loss: 1.019\n","[Epoch 3, Batch   200] loss: 1.000\n","[Epoch 3, Batch   300] loss: 0.982\n","[Epoch 3, Batch   400] loss: 0.990\n","[Epoch 3, Batch   500] loss: 0.976\n","[Epoch 3, Batch   600] loss: 0.992\n","[Epoch 3, Batch   700] loss: 0.960\n","[Epoch 4, Batch   100] loss: 0.886\n","[Epoch 4, Batch   200] loss: 0.893\n","[Epoch 4, Batch   300] loss: 0.888\n","[Epoch 4, Batch   400] loss: 0.901\n","[Epoch 4, Batch   500] loss: 0.905\n","[Epoch 4, Batch   600] loss: 0.900\n","[Epoch 4, Batch   700] loss: 0.884\n","[Epoch 5, Batch   100] loss: 0.831\n","[Epoch 5, Batch   200] loss: 0.822\n","[Epoch 5, Batch   300] loss: 0.824\n","[Epoch 5, Batch   400] loss: 0.800\n","[Epoch 5, Batch   500] loss: 0.841\n","[Epoch 5, Batch   600] loss: 0.822\n","[Epoch 5, Batch   700] loss: 0.837\n","[Epoch 6, Batch   100] loss: 0.760\n","[Epoch 6, Batch   200] loss: 0.762\n","[Epoch 6, Batch   300] loss: 0.772\n","[Epoch 6, Batch   400] loss: 0.779\n","[Epoch 6, Batch   500] loss: 0.794\n","[Epoch 6, Batch   600] loss: 0.772\n","[Epoch 6, Batch   700] loss: 0.777\n","[Epoch 7, Batch   100] loss: 0.675\n","[Epoch 7, Batch   200] loss: 0.711\n","[Epoch 7, Batch   300] loss: 0.724\n","[Epoch 7, Batch   400] loss: 0.714\n","[Epoch 7, Batch   500] loss: 0.743\n","[Epoch 7, Batch   600] loss: 0.735\n","[Epoch 7, Batch   700] loss: 0.741\n","[Epoch 8, Batch   100] loss: 0.653\n","[Epoch 8, Batch   200] loss: 0.654\n","[Epoch 8, Batch   300] loss: 0.659\n","[Epoch 8, Batch   400] loss: 0.688\n","[Epoch 8, Batch   500] loss: 0.706\n","[Epoch 8, Batch   600] loss: 0.667\n","[Epoch 8, Batch   700] loss: 0.700\n","[Epoch 9, Batch   100] loss: 0.610\n","[Epoch 9, Batch   200] loss: 0.625\n","[Epoch 9, Batch   300] loss: 0.616\n","[Epoch 9, Batch   400] loss: 0.632\n","[Epoch 9, Batch   500] loss: 0.666\n","[Epoch 9, Batch   600] loss: 0.651\n","[Epoch 9, Batch   700] loss: 0.646\n","[Epoch 10, Batch   100] loss: 0.574\n","[Epoch 10, Batch   200] loss: 0.578\n","[Epoch 10, Batch   300] loss: 0.610\n","[Epoch 10, Batch   400] loss: 0.593\n","[Epoch 10, Batch   500] loss: 0.611\n","[Epoch 10, Batch   600] loss: 0.609\n","[Epoch 10, Batch   700] loss: 0.616\n"]}]},{"cell_type":"markdown","metadata":{"id":"iesLEmXFQNOc"},"source":["## Prediction"],"id":"iesLEmXFQNOc"},{"cell_type":"markdown","source":["## Evaluation"],"metadata":{"id":"4_o8Rf5-NLaZ"},"id":"4_o8Rf5-NLaZ"},{"cell_type":"code","source":["# Test the model\n","model.eval()\n","correct_test = 0\n","total_test = 0\n","y_true = []\n","y_pred = []\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        outputs = model(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        y_true += labels.tolist()\n","        y_pred += predicted.tolist()\n","        total_test += labels.size(0)\n","        correct_test += (predicted == labels).sum().item()\n","\n","        # Compute metrics and print results\n","        acc_test = correct_test / total_test\n","        print('Accuracy on test set: %.3f' % acc_test)"],"metadata":{"id":"Y_iSitADaNEh"},"id":"Y_iSitADaNEh","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute the confusion matrix and classification report\n","from sklearn.metrics import confusion_matrix, classification_report\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","cm = confusion_matrix(y_true, y_pred)\n","report = classification_report(y_true, y_pred, target_names=classes)\n","\n","print(\"Confusion Matrix:\")\n","print(cm)\n","print(\"Classification Report:\")\n","print(report)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-d_n57-tcW13","executionInfo":{"status":"ok","timestamp":1694221359882,"user_tz":-180,"elapsed":433,"user":{"displayName":"Abdelrahman Katkat","userId":"13800345600658892031"}},"outputId":"78002c56-925f-4462-cea9-d10b4839d3a3"},"id":"-d_n57-tcW13","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Confusion Matrix:\n","[[725  22  55  18  19  10  15  10  87  39]\n"," [ 23 777   9  11   7   6  14   9  29 115]\n"," [ 62   4 579  43  89  62  89  43  16  13]\n"," [ 18   7  93 441  74 181 107  39  28  12]\n"," [ 15   3  91  43 645  39  79  64  18   3]\n"," [  9   4  62 141  56 623  33  51  14   7]\n"," [  4   2  41  26  31  28 846   8  10   4]\n"," [ 11   4  35  27  81  61   8 748   5  20]\n"," [ 50  36   9   9  16   9   8   3 833  27]\n"," [ 30  76  11  17  15   6  14  24  45 762]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","       plane       0.77      0.72      0.74      1000\n","         car       0.83      0.78      0.80      1000\n","        bird       0.59      0.58      0.58      1000\n","         cat       0.57      0.44      0.50      1000\n","        deer       0.62      0.65      0.63      1000\n","         dog       0.61      0.62      0.62      1000\n","        frog       0.70      0.85      0.76      1000\n","       horse       0.75      0.75      0.75      1000\n","        ship       0.77      0.83      0.80      1000\n","       truck       0.76      0.76      0.76      1000\n","\n","    accuracy                           0.70     10000\n","   macro avg       0.70      0.70      0.70     10000\n","weighted avg       0.70      0.70      0.70     10000\n","\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"pHHR7z47cWNJ"},"id":"pHHR7z47cWNJ"},{"cell_type":"code","execution_count":null,"metadata":{"id":"IKlhvLpVQNOd"},"outputs":[],"source":[],"id":"IKlhvLpVQNOd"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}